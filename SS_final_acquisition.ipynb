{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pandas.errors import EmptyDataError\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import re\n",
    "from Bio.Alphabet import IUPAC\n",
    "import subprocess\n",
    "from collections import OrderedDict\n",
    "import os, os.path\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "from Bio.SubsMat.MatrixInfo import blosum62\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "#Chrome Driver imports\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from selenium.webdriver.support.ui import WebDriverWait # available since 2.4.0\n",
    "from selenium.webdriver.support import expected_conditions as EC # available since 2.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"File and directory management functions\"\"\"\n",
    "def create_directory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "def empty_directory(path):\n",
    "    for i in glob.glob(os.path.join(path, '*')):\n",
    "        if os.path.isdir(i):\n",
    "            shutil.rmtree(i)\n",
    "        else:\n",
    "            os.remove(i)\n",
    "\n",
    "def create_run_directory(run_name):\n",
    "    \"\"\"Make diretory tree for a run.\"\"\"\n",
    "    dirpaths = [\"{0}\",\"{0}/input\",\"{0}/input/ODB\",\"{0}/input/NCBI\",\"{0}/output\",\"{0}/summary\",\"{0}/run_params\"]\n",
    "    for dirpath in dirpaths:\n",
    "        formatted = dirpath.format(run_name)\n",
    "        create_directory(formatted)\n",
    "\n",
    "def write_run_params_file(config, spec_path, spec_hc):\n",
    "    \"\"\"Documents some run specific parameters. \n",
    "    \n",
    "    config: config file object storing some run specifications (directory names, file paths) \n",
    "    spec_path: file path for the input species list being used for analysis \n",
    "    spec_hc: hashcode generated from species list. \n",
    "    \"\"\"\n",
    "    config_keys = [\"RunName\",\"GenesFile\",\"odb_level\"]\n",
    "    run_name = config[\"RunName\"]\n",
    "    fpath = run_name+\"/run_params/params.txt\"\n",
    "    params_f = open(fpath, 'wt')\n",
    "    for key in config_keys:\n",
    "        val = config[key]\n",
    "        file_line = \"{0}: {1}\\n\".format(key,val)\n",
    "        params_f.write(file_line)\n",
    "    params_f.write(\"species_list: {0}\\n\".format(spec_path))\n",
    "    params_f.write(\"species_hashcode: {0}\\n\".format(spec_hc))\n",
    "    params_f.close()\n",
    "\n",
    "#tmp directory is used to store any run intermediate files (sequence filtering results)\n",
    "create_directory(\"tmp\")\n",
    "empty_directory('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error classes for OrthoDB query failures, missing sequence data, or GeneCards queries\n",
    "# class Error(Exception):\n",
    "#     \"\"\"Base class for exceptions in this module.\"\"\"\n",
    "#     pass\n",
    "class SequenceDataEx(Error):\n",
    "    \"\"\"Error class to be raised if missing GS sequences from OrthoDB/ NCBI data\"\"\"\n",
    "    error_type = \"SequenceDataError\"\n",
    "    def __init__(self,code, message):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "class OrthoDBQueryError(Error):\n",
    "    \"\"\"Error class to be raised if OrthoDB query failed to generate input files\"\"\"\n",
    "    error_type = \"OrthoDBQueryError\"\n",
    "    def __init__(self, code, message):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "class GeneCardsError(Error):\n",
    "    \"\"\"Error class if aliases for a gene symbol could not be automatically fetched\"\"\"\n",
    "    error_type = \"GeneCardsError\"\n",
    "    def __init__(self, code, message):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "class SequenceAnalysisError(Error):\n",
    "    \"\"\"Error class if JSD/ BLOSUM metrics analysis cannot be completed for a gene\"\"\"\n",
    "    error_type = \"SequenceAnalysisError\"\n",
    "    def __init__(self, code, message):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "\n",
    "        \n",
    "def write_errors(errors_fpath,gene_name,error):\n",
    "    \"\"\"Maintains a tsv file of gene symbols and errors generated during the run.\n",
    "    \"\"\"\n",
    "    error_type = error.error_type\n",
    "    error_code = error.code \n",
    "    error_msg = error.message\n",
    "    if not os.path.exists(errors_fpath):\n",
    "        errors_f = open(errors_fpath,'wt')\n",
    "        errors_f.write(\"gene\\terror_type\\terror_code\\terror_str\\n\")\n",
    "    else:\n",
    "        errors_df = pd.read_csv(errors_fpath,delimiter='\\t')\n",
    "        if gene_name in errors_df[\"gene\"].unique():\n",
    "            gene_error_df = errors_df.loc[errors_df[\"gene\"]==gene_name,:]\n",
    "            if gene_error_df[\"error_str\"].str.contains(error_msg).any():\n",
    "#                 print(\"Previously stored error:\")\n",
    "                error_row = gene_error_df.loc[gene_error_df[\"error_str\"]==error_msg,:]\n",
    "                gene_name,error_type,error_code,error_msg = error_row.values[0]\n",
    "                print(\"{0}\\t{1}\\t{2}\\t{3}\".format(gene_name,error_type,error_code,error_msg))\n",
    "                return\n",
    "    errors_f = open(errors_fpath,'at')\n",
    "    fline = \"{0}\\t{1}\\t{2}\\t{3}\\n\".format(gene_name,error_type,error_code,error_msg)\n",
    "    errors_f.write(fline)\n",
    "    print(fline)\n",
    "    errors_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcitons for reading config files, including run parameters, gene symbols list, and species lists. \n",
    "\n",
    "def parse_config(config_file=\"config/config.txt\"):\n",
    "    \"\"\"Parse config text file (INI format) to establish paramters for the run\n",
    "    \n",
    "    config_file: path to the config file (\"config/config.txt\" by default)\n",
    "    \"\"\"\n",
    "    import configparser\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    return config[\"DEFAULT\"]\n",
    "def parse_genes(genes_path=\"config/genes.txt\"):\n",
    "    \"\"\"Parses gene file into list of uppercase, whitespace trimmed gene names\"\"\"\n",
    "    gene_flines = open(genes_path).readlines()\n",
    "    genes = [gene.strip().upper() for gene in gene_flines]\n",
    "    return genes\n",
    "def parse_species(species_path=\"config/v10_0_species.txt\"):\n",
    "    #Reads species list from file in config directory. Also returns a hashcode for the list of species used\n",
    "    spec_lines = open(species_path).readlines()\n",
    "    species = [spec.strip() for spec in spec_lines]\n",
    "    concat = \"\"\n",
    "    for spec in species: \n",
    "        concat = concat + spec\n",
    "    hc = np.abs(hash(concat))\n",
    "    return species, hc\n",
    "\n",
    "def odb_tablev9(species_list,table_path=\"odb9v1_raw/odb9v1_species.tab\"):\n",
    "    \"\"\"Reads orthodb v9 tsv file into a DataFrame of species names/ tax_ids and other ODB information.\n",
    "        Mainly used for taxid <-> species name conversions\n",
    "    \"\"\"\n",
    "    odb = pd.read_csv(table_path,delimiter=\"\\t\",header=None,names=[\"tax_id\",\"odb_id\",\"spec_name\",\"clustered_genes\",\"ortho_groups\",\"mapping_type\"])\n",
    "    filtered = pd.DataFrame(columns=odb.columns)\n",
    "    for spec in species_list:\n",
    "        row = odb[odb[\"spec_name\"]==spec]\n",
    "        filtered = filtered.append(row)\n",
    "    filtered.drop(columns=[\"clustered_genes\",\"ortho_groups\",\"mapping_type\"],inplace=True)\n",
    "    return filtered\n",
    "def odb_tablev10(species_list,table_path=\"config/odb10v0_species.tab\"):\n",
    "    \"\"\"odb10v0_species.tab:\n",
    "    1.\tNCBI tax id\n",
    "    2.\tOrtho DB individual organism id, based on NCBI tax id\n",
    "    3.\tscientific name inherited from the most relevant NCBI tax id\n",
    "    4.\tgenome asssembly id, when available\n",
    "    5.\ttotal count of clustered genes in this species\n",
    "    6.\ttotal count of the OGs it participates\n",
    "    7.\tmapping type, clustered(C) or mapped(M)\n",
    "    Reads above file into a DataFrame used for tax_id/ species name information \n",
    "    \"\"\"\n",
    "    odb = pd.read_csv(table_path,delimiter=\"\\t\",header=None,names=[\"tax_id\",\"odb_id\",\"spec_name\",\"assembly_id\",\"clustered_genes\",\"ortho_groups\",\"mapping_type\"])\n",
    "    filtered = pd.DataFrame(columns=odb.columns)\n",
    "    for spec in species_list:\n",
    "        row = odb[odb[\"spec_name\"]==spec]\n",
    "        filtered = filtered.append(row)\n",
    "    filtered.drop(columns=[\"clustered_genes\",\"ortho_groups\",\"mapping_type\"],inplace=True)\n",
    "    return filtered\n",
    "\n",
    "#Read config files \n",
    "config = parse_config()\n",
    "test_species = config[\"TestSpecies\"]\n",
    "species_path=\"config/v10_0_species.txt\"\n",
    "spec_list, hc = parse_species(species_path)\n",
    "gene_list = parse_genes(config[\"GenesFile\"])\n",
    "tax_table = odb_tablev10(spec_list)\n",
    "run_name = config[\"RunName\"]\n",
    "create_run_directory(run_name)\n",
    "\n",
    "errors_fpath = '{0}/summary/errors.tsv'.format(run_name)\n",
    "seq_qc_fpath = '{0}/summary/seq_QC.tsv'.format(run_name)\n",
    "\n",
    "DISPLAY_PARAMS = False\n",
    "if DISPLAY_PARAMS:\n",
    "    print(\"Tax table for species list at \"+species_path)\n",
    "    display(tax_table)\n",
    "    print(\"Gene list: \"+str(gene_list))\n",
    "    print(\"Run Name: \"+ run_name)\n",
    "#Verify that species table, gene list, and run_name are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_blos_df():\n",
    "    \"\"\"\n",
    "    Generates 1) a list of accepted amino acid characters, 2) a DataFrame corresponding to the BLSOUM62 matrix\n",
    "    that can be indexed using non-gap amino acid characters on rows/ cols, and 3) a dictionary from non-gap\n",
    "    amino acid characters to background probabilities from the BLOSUM62 matrix\n",
    "    Uses BLOSUM background distribution from Capra and Singh (2007)\n",
    "    \"\"\"\n",
    "    global aas, blosum62_bg\n",
    "    aas = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V','-']\n",
    "    bg_probs = [0.078, 0.051, 0.041, 0.052, 0.024, 0.034, 0.059, 0.083, 0.025, 0.062, 0.092, 0.056, 0.024, 0.044, 0.043, 0.059, 0.055, 0.014, 0.034, 0.072]#, 0.000001]\n",
    "    blosum62_bgdict = dict(zip(aas,bg_probs))\n",
    "    blosum62_bg = bg_probs\n",
    "    blos_df = pd.DataFrame(index=aas[:-1],columns=aas[:-1])\n",
    "    for pair in blosum62:\n",
    "        val = blosum62[pair]\n",
    "        first, second = pair[0],pair[1]\n",
    "        if first in aas and second in aas:\n",
    "            blos_df.loc[first,second] = val\n",
    "            blos_df.loc[second,first] = val\n",
    "    sim_matrix = blos_df.values\n",
    "    return aas, blosum62_bg, blos_df, sim_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acquire input data via OrthoDB API \n",
    "def ODB_query(run_name,gene_name,level_str,spec_str):\n",
    "    \"\"\"Queries OrthoDB via the fasta and tab API for gene_name. \n",
    "    More info: https://www.orthodb.org/orthodb_userguide.html#api\n",
    "    level_str corresponds to the API variable for phylogenetic clade \n",
    "    spec_str corresponds to the taxonomy ids for the list of species from the config folder \n",
    "    \"\"\"\n",
    "    import time, json \n",
    "    from json import JSONDecodeError\n",
    "    #File paths and OrthoDB urls for downloads. NOTE BASE_URL might need updating depending on ODB conventions\n",
    "    BASE_URL = \"https://v101.orthodb.org\"\n",
    "    query_str = \"query={0}\".format(gene_name)\n",
    "    fasta_url = \"{0}/fasta?{1}&{2}&{3}\".format(BASE_URL,query_str,level_str,spec_str)\n",
    "    fasta_path = \"{0}/input/{1}.fasta\".format(run_name,gene_name)\n",
    "    tsv_url = \"{0}/tab?{1}&{2}&{3}\".format(BASE_URL,query_str,level_str,spec_str)\n",
    "    tsv_path = \"{0}/input/{1}.tsv\".format(run_name,gene_name)\n",
    "    #Obey OrthoDB download restrictions (one request per second) bc you're a good noodle\n",
    "    t1 = time.process_time()\n",
    "    fasta_proc = subprocess.run(args=['wget',fasta_url,'-O',fasta_path])\n",
    "    if (time.process_time()-t1) < 1:\n",
    "        time.sleep(0.5) \n",
    "    t1 = time.process_time()\n",
    "    tsv_proc = subprocess.run(args=['wget',tsv_url,'-O',tsv_path])\n",
    "    if (time.process_time()-t1) < 1:\n",
    "        time.sleep(0.5)\n",
    "    try: \n",
    "        #JSON format returned if no results for query string - try opening downloaded data as JSON, if\n",
    "        #successful, raise an OrthoDBQueryError\n",
    "        tsv_json = json.load(open(tsv_path))\n",
    "        os.remove(fasta_path)\n",
    "        os.remove(tsv_path)\n",
    "        raise OrthoDBQueryError(0,\"No OrthoDB results for query\")\n",
    "    except JSONDecodeError:\n",
    "        #Check if html syntax present in file (result of too many clusters returned to be downloaded); \n",
    "        #if not, query was successful and run_name/input should now have ODB formatted .fasta and .tsv files\n",
    "        file_txt = \"\"\n",
    "        with open(fasta_path,\"rt\") as fasta_f:\n",
    "            for i in range(10):\n",
    "                file_txt = file_txt + fasta_f.readline()\n",
    "            if bool(BeautifulSoup(file_txt,\"html.parser\").find()):\n",
    "                os.remove(fasta_path)\n",
    "                os.remove(tsv_path)\n",
    "                raise OrthoDBQueryError(1,\"OrthoDB search yielded too many clusters\")\n",
    "            #If no OrthoDBQueryError is raised, download was successful (no further action needed)\n",
    "    \n",
    "def download_input_data(gene_list,tax_table,config):\n",
    "    \"\"\"Queries OrthoDB for all entries in gene list (logs failed searches into errors_fpath), using species \n",
    "    list from tax_table and taxonomy level provided in config directory. This function will attempt to \n",
    "    query OrthoDB for each gene symbol in gene_list according to the species list in the config directory. \n",
    "    Note that for configuring the species list, one OrthoDB input file can be generated with a large \n",
    "    number of species whose sequences can later be removed from the sequence set used for alignment/ analysis,\n",
    "    allowing the same input file to serve different downstream analyses with smaller species sets. \n",
    "    \n",
    "    Returns the list of gene symbols from gene_list for which OrthoDB data was successfully downloaded\n",
    "    and the list of gene symbols for which the OrthoDB queries failed\"\"\"\n",
    "    tax_ids = tax_table[\"tax_id\"].values.astype(str)\n",
    "    spec_str = \"species=\"+\",\".join(tax_ids)\n",
    "    level_str = \"level=\"+str(config[\"odb_level\"])\n",
    "    failed_queries = []\n",
    "    run_name = config[\"RunName\"]\n",
    "    errors_fpath = run_name+\"/summary/errors.tsv\"\n",
    "    if os.path.exists(errors_fpath):\n",
    "        errors_df = pd.read_csv(errors_fpath,delimiter='\\t')\n",
    "        ODB_errors_df = errors_df.loc[errors_df[\"error_type\"]==\"OrthoDBQueryError\",:]\n",
    "        check_error_file = True\n",
    "    else:\n",
    "        check_error_file = False\n",
    "    for gene_name in gene_list: \n",
    "        fasta_path = \"{0}/input/{1}.fasta\".format(run_name,gene_name)\n",
    "        if config.getboolean(\"OverwriteInput\") or not os.path.exists(fasta_path):\n",
    "            if check_error_file and gene_name in ODB_errors_df[\"gene\"].unique():#ODB_errors_df[\"gene\"].str.match(gene_name).any():\n",
    "                ODB_error_row = ODB_errors_df.loc[ODB_errors_df[\"gene\"]==gene_name,:]\n",
    "                genename,error_type,error_code,error_msg = ODB_error_row.values[0]\n",
    "                print(\"{0}\\t{1}\\t{2}\\t{3}\".format(genename,error_type,error_code,error_msg))\n",
    "                failed_queries.append(gene_name)\n",
    "            else:\n",
    "                try:\n",
    "                    ODB_query(run_name,gene_name,level_str,spec_str)\n",
    "                except OrthoDBQueryError as odb_error:\n",
    "                    failed_queries.append(gene_name)\n",
    "                    write_errors(errors_fpath,gene_name,odb_error)\n",
    "\n",
    "    print(\"Input queries downloaded.\")\n",
    "    valid_queries = [gene for gene in gene_list if gene not in failed_queries]\n",
    "    return valid_queries, failed_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_AGS_data(input_csv_fpath, config):\n",
    "    run_name = config[\"RunName\"]\n",
    "    NCBI_input_dir = \"{0}/input/NCBI\".format(run_name)\n",
    "    create_directory(AGS_input_dir)\n",
    "    \n",
    "    unfilled_ID_csv_fpath = \"config/cDNA_list_AGS_geneIDs_edited.csv\"\n",
    "    filled_outpath = \"{0}/summary/cDNA_list_AGS_geneIDs_complete.csv\".format(run_name)\n",
    "    NCBI_errors_fpath = \"{0}/summary/NCBI_errors.tsv\".format(run_name)\n",
    "    \n",
    "    geneID_df = map_AGS_geneIDs(unfilled_ID_csv_fpath,filled_outpath,NCBI_errors_fpath)\n",
    "    AGS_gene_id_df = geneID_df.loc[~geneID_df[field_name].isnull(),:]\n",
    "    AGS_gene_id_df = download_NCBI_records(AGS_gene_id_df,NCBI_input_dir)\n",
    "    \n",
    "def map_AGS_geneIDs(csv_inpath, results_outpath, errors_tsv_path):\n",
    "    \"\"\"Reads a csv file at specified path, generates a new DataFrame containing AGS Gene IDs when available.\n",
    "    \n",
    "    csv_inpath: csv file containing list of genes and other identifying information. Only required columns are\n",
    "    \"Human Gene ID\", and FIELD_NAME (AGS Gene ID). \n",
    "    For each row in the csv file, attempts to fetch NCBI Gene ID for AGS using ortholog information. Will\n",
    "    fail if 1) Human Gene ID is missing or 2) No orthologs could be fetched for AGS and will write a brief\n",
    "    error message to errors_tsv.\n",
    "    Returns a DataFrame object corresponding to the original csv file with all possible entries of FIELD_NAME\n",
    "    populated with the ortholog Gene ID numbers. \n",
    "    \n",
    "    To repeat this sequence fetching for another species, adjust AGS_TAX_ID to the corresponding species' \n",
    "    NCBI Taxonomy ID (can be found on Taxonomy Browser)\n",
    "    \"\"\"\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    chrome_options = Options()  \n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    WINDOW_SIZE = \"1920,1080\"\n",
    "    chrome_options.add_argument(\"--window-size=%s\" % WINDOW_SIZE)\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "\n",
    "    FIELD_NAME = \"AGS Gene ID\"\n",
    "    AGS_TAX_ID = \"9999\"\n",
    "    field_conv_dict = {\"Human Gene ID\":str,FIELD_NAME:str}\n",
    "    csv_df = pd.read_csv(csv_inpath,dtype=field_conv_dict)\n",
    "\n",
    "    missing_AGS_gid = csv_df.loc[csv_df[FIELD_NAME].isnull(),:]\n",
    "    missing_hgid = missing_AGS_gid.loc[missing_AGS_gid[\"Human Gene ID\"].isnull(),:]\n",
    "\n",
    "    if os.path.exists(errors_tsv_path):\n",
    "        errors_df = pd.read_csv(errors_tsv_path,delimiter=\"\\t\",index_col=\"gene\")\n",
    "        \n",
    "    for idx,row in missing_AGS_gid.iterrows():\n",
    "        symbol = row[\"Gene Symbol\"]\n",
    "        hgid = row[\"Human Gene ID\"]\n",
    "        if idx in missing_hgid.index:\n",
    "            write_errors(symbol,\"No Human GeneID present in data\",errors_tsv_path)\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                req_url = \"https://www.ncbi.nlm.nih.gov/gene/{0}/ortholog/?scope={1}\".format(hgid,AGS_TAX_ID)\n",
    "                driver.get(req_url)\n",
    "                result_url = driver.current_url\n",
    "                if re.search(\"scope={0}\".format(AGS_TAX_ID),result_url):\n",
    "                    entry_xpath = \"//tbody/tr/td[@class=' fld-gene']/a\"\n",
    "                    entry = driver.find_element_by_xpath(entry_xpath)\n",
    "                    entry_href = entry.get_attribute('href')\n",
    "                    entry_gid = re.search(\"/gene/(\\d*)\",entry_href).groups()[0]\n",
    "                    csv_df.loc[idx, FIELD_NAME] = entry_gid\n",
    "                else:\n",
    "                    write_errors(symbol,\"No AGS ortholog present for GeneID {0}\".format(hgid),errors_tsv_path)\n",
    "            except NoSuchElementException as e:\n",
    "                write_errors(symbol,\"No Orthologs present for GeneID {0}\".format(hgid),errors_tsv_path)\n",
    "    \n",
    "    csv_df.to_csv(results_outpath)\n",
    "    return csv_df\n",
    "\n",
    "def download_NCBI_records(AGS_gene_id_df, NCBI_records_dirpath):\n",
    "    \"\"\"Downloads NCBI protein records for each NCBI Gene ID listed in AGS_gene_id_df.\n",
    "    \n",
    "    AGS_gene_id_df: DataFrame object with required columns 'Gene Symbol' and 'AGS Gene ID.' Gene symbol \n",
    "    entries are used to name fasta files downloaded; AGS Gene IDs are queried using Entrez elink \n",
    "    to 1) match Gene ID to all corresponding Protein IDs and 2) download those Protein IDs into one fasta \n",
    "    file per gene symbol, saved into NCBI_records_dirpath\n",
    "    \n",
    "    Adds the comma-separated list of NCBI Protein record IDs to AGS_gene_id_df in the column \"Prot_UIDs\"\n",
    "    and returns AGS_gene_id_df\n",
    "    \"\"\"\n",
    "    ENTREZ_BASE_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "    NCBI_API_KEY = \"24a8e1dd4d64cf50b37f0bdd369af8274309\"\n",
    "    OVERWRITE_FASTAS = [\"ATPIF1\"]\n",
    "    #Convert Gene ID to list of Protein IDs corresponding to transcript variant sequences\n",
    "    # for AGS_gid in AGS_gids[:1]:\n",
    "    for idx,row in AGS_gene_id_df.iterrows():\n",
    "        symbol = row[\"Gene Symbol\"]\n",
    "        print(\"==={0}===\".format(symbol))\n",
    "        fasta_fpath = \"{0}/{1}_AGS.fasta\".format(NCBI_records_dirpath,symbol)\n",
    "        if not os.path.exists(fasta_fpath) or symbol in OVERWRITE_FASTAS:\n",
    "            AGS_gid = row[field_name]\n",
    "            print(\"AGS Gene ID: {0}\".format(AGS_gid))\n",
    "            elink_req = \"elink.fcgi?dbfrom=gene&db=protein&id={0}&api_key={1}\".format(AGS_gid,NCBI_API_KEY)\n",
    "            gp_elink_url = ENTREZ_BASE_URL+elink_req\n",
    "\n",
    "            file = urllib.request.urlopen(gp_elink_url)\n",
    "            xml_data = file.read()\n",
    "            file.close()\n",
    "\n",
    "            root = ET.fromstring(xml_data)\n",
    "            #Check XML formatting of elink pages - update xpath accordingly if functionality breaks\n",
    "            #Pulls Record IDs for Protein specifically; use gene_protein_refseq for Protein RefSeqs\n",
    "            protein_IDs = [link.text for link in root.findall(\".//LinkSetDb[LinkName='gene_protein']/Link/Id\")]\n",
    "            id_str = ','.join(protein_IDs)\n",
    "            AGS_gene_id_df.loc[idx,\"Prot_UIDs\"] = id_str    \n",
    "\n",
    "            efetch_req = \"efetch.fcgi?db=protein&id={0}&rettype=fasta&retmode=text&api_key={1}\".format(id_str,NCBI_API_KEY)\n",
    "            efetch_url = ENTREZ_BASE_URL + efetch_req\n",
    "            subprocess.run(args=['wget',efetch_url,'-O',fasta_fpath])\n",
    "    return AGS_gene_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_odb_field(field):\n",
    "    \"\"\"Remove spaces, commas, and capitalization from alias/ odb fields to search for string matches.\n",
    "    If field is empty (np.nan), return empty string\"\"\"\n",
    "    if(type(field)) == str:\n",
    "        field = field.replace(\" \",\"\")\n",
    "        field = field.replace(\",\",\"\")\n",
    "        field = field.replace(\"\\n\",\"\")\n",
    "        return field.lower()\n",
    "    elif type(field) == float and np.isnan(field):\n",
    "        return \"\"\n",
    "    \n",
    "def write_aliases_f(aliases,aliases_fpath):\n",
    "    \"\"\"Write aliases data from GeneCards to a txt file list at aliases_fpath\"\"\"\n",
    "    aliases_f = open(aliases_fpath,'wt')\n",
    "    for a in aliases:\n",
    "        aliases_f.write(a.strip()+'\\n')\n",
    "    aliases_f.close()\n",
    "    \n",
    "def download_alias_data(gene_name):\n",
    "    \"\"\"Queries GeneCards for alias data for gene_name. Should only be called if aliases_fpath doesn't exist \n",
    "    (ie if query has not been previously run and written to file). Attempts GeneCards query - if gene_name\n",
    "    leads to a single entry page, pulls aliases from page html. If query leads to a query results page, \n",
    "    checks all linked entries to see if any contain gene_name. If none do (or other WebDriver issues arise),\n",
    "    raises a GeneCardsError. \n",
    "    \n",
    "    If query was successful (either single result page or successfully chose linked result from query results),\n",
    "    return aliases and gc_name (the gene identifier used by GeneCards). gc_name stored separately since alias html \n",
    "    extraction will miss it otherwise. Also writes alias data to aliases_fpath\n",
    "    \n",
    "    Updated 01/10/2020. If function is consistently failing, check xpath class names against orthodb website\"\"\"\n",
    "    aliases_fpath = \"aliases_data/\"+gene_name+\"_aliases.txt\"\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    chrome_options = Options()  \n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    WINDOW_SIZE = \"1920,1080\"\n",
    "    chrome_options.add_argument(\"--window-size=%s\" % WINDOW_SIZE)\n",
    "    import time \n",
    "    gene_cards_url = \"https://www.genecards.org/cgi-bin/carddisp.pl?gene={0}\".format(gene_name.upper())\n",
    "    list_xpath = \"//ul[@class='list-unstyled list-spacious']/li\"\n",
    "    elem_xpaths = [list_xpath]\n",
    "    driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    driver.get(gene_cards_url)\n",
    "    aliases = []\n",
    "    for xpath in elem_xpaths:\n",
    "        elems = driver.find_elements_by_xpath(xpath)\n",
    "        innerHTMLs = [elem.get_attribute(\"innerHTML\") for elem in elems]\n",
    "        col_aliases = [BeautifulSoup(markup).find(text=True).strip() for markup in innerHTMLs]\n",
    "        aliases.extend(col_aliases)\n",
    "    if len(aliases) > 0:  \n",
    "        #Means gene_name query to GeneCards autoredirected to a single page - normal aliases scraping\n",
    "        #HTML parsing for GeneCards website - end result is list of trimmed alias strings\n",
    "        gc_re = re.search(\"gene=([A-Z0-9]+)\",gene_cards_url)\n",
    "        gc_name = gc_re.groups()[0].strip()\n",
    "        if gc_name not in aliases:\n",
    "            aliases.insert(0,gc_name)\n",
    "        #Cache aliases to aliases_fpath\n",
    "        write_aliases_f(aliases,aliases_fpath)\n",
    "        driver.quit()\n",
    "        return aliases, gc_name\n",
    "    else: \n",
    "        #Try search results page for gene_name; raise GeneCardsError if no results or check each page \n",
    "        #for alias matching gene_name otherwise\n",
    "        query_url = \"https://www.genecards.org/Search/Keyword?queryString={0}\".format(gene_name)\n",
    "        links_xpath = \"//td[@class='gc-gene-symbol gc-highlight symbol-col']/a\"\n",
    "        link_elems = driver.find_elements_by_xpath(links_xpath)\n",
    "        if link_elems: \n",
    "            for elem in links_elems:\n",
    "                elem_href = elem.get_attribute(\"href\")\n",
    "                driver.get(elem_href)\n",
    "                query_url = driver.current_url\n",
    "                elem_gc_name = re.search(\"gene=([A-Z0-9]+)\",query_url).groups()[0].strip()\n",
    "                elem_aliases = []\n",
    "                for xpath in elem_xpaths:\n",
    "                    elems = driver.find_elements_by_xpath(xpath)\n",
    "                    innerHTMLs = [elem.get_attribute(\"innerHTML\") for elem in elems]\n",
    "                    col_aliases = [BeautifulSoup(markup).find(text=True).strip() for markup in innerHTMLs]\n",
    "                    elem_aliases.extend(col_aliases)\n",
    "                if gene_name in elem_aliases or gene_name == elem_gc_name:\n",
    "                    #Found query result with gene_name \n",
    "                    driver.quit()\n",
    "                    if elem_gc_name not in elem_aliases:\n",
    "                        elem_aliases.insert(0,elem_gc_name)\n",
    "                    write_aliases_f(elem_aliases,aliases_fpath)\n",
    "                    return elem_aliases, elem_gc_name\n",
    "        #If either no link_elems (empty search results page), or none correspond to gene_name:\n",
    "        driver.quit()\n",
    "        raise GeneCardsError(0,\"Could not automatically fetch alias data from GeneCards - consider searching manually\")\n",
    "\n",
    "def find_ref_seqs(gene_name, tsv_df,errors_fpath):\n",
    "    \"\"\"Returns a list of the orthodb ids of the reference sequences from an OrthoDB tsv_df and a set containing \n",
    "    gene_name and the GeneCards primary alias for gene_name (if it differs from gene_name)\n",
    "    These reference sequences are defined as records with pub_gene_id, og_name, or description having a\n",
    "    text match to either gene_name or one of the GeneCards listed aliases for gene_name. \n",
    "    Alias data is fetched from GeneCards automatically and stored in the aliases directory as text file lists. \n",
    "    \"\"\" \n",
    "    ref_ids = []\n",
    "    #Go to genecards page for gene_name, extract information for aliases from the webpage/ html \n",
    "    aliases_fpath = \"aliases_data/\"+gene_name+\"_aliases.txt\"\n",
    "    \n",
    "    #If file doesn't exist or was improperly downloaded to yield only one line, repeat \n",
    "    #fetching alias names \n",
    "    if (not os.path.exists(aliases_fpath)) or len(open(aliases_fpath,'r').readlines()) == 1:\n",
    "        try: \n",
    "            aliases, gc_name = download_alias_data(gene_name)\n",
    "            matches = set((gene_name.upper(),gc_name.upper()))\n",
    "        except GeneCardsError as gc_error:\n",
    "            aliases = [gene_name]\n",
    "            matches = set((gene_name.upper(),))\n",
    "            write_errors(errors_fpath,gene_name,gc_error)\n",
    "    else:\n",
    "        #Read aliases information previously downloaded from GeneCards\n",
    "        aliases_f = open(aliases_fpath,'r')\n",
    "        aliases = aliases_f.readlines()\n",
    "        aliases_f = open(aliases_fpath,'r')\n",
    "        gc_name = aliases_f.readline().strip()\n",
    "        matches = set((gene_name.upper(),gc_name.upper()))\n",
    "    #Remove spaces, commas, new line chars, and capitalization from alias strings\n",
    "    formatted_aliases = [format_odb_field(alias) for alias in aliases]\n",
    "    #Search fields in search_fields for matches to the alias strings provided by GeneCards\n",
    "    #Iterate tsv_df rows, save all reference ids which have matches \n",
    "    search_fields = [\"pub_gene_id\",\"og_name\",\"description\"]\n",
    "    aliases_pat = \"|\".join(formatted_aliases)\n",
    "    for idx,row in tsv_df.iterrows():\n",
    "#         for field in search_fields:\n",
    "            #Current behavior: exact matches in formatted pub_gene_id, og_name, or description only.\n",
    "            #TODO: Add in partial string matching. Difficulties with distinguishing gene_names\n",
    "        for alias in formatted_aliases: \n",
    "            for field in search_fields:\n",
    "                formatted_field = format_odb_field(str(row[field]))\n",
    "                try:\n",
    "                    if re.search(alias,formatted_field):\n",
    "                        if idx not in ref_ids:\n",
    "                            ref_ids.append(idx)#[\"int_prot_id\"])\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    #Special regexp characters present in alias \n",
    "                    if alias in formatted_field:\n",
    "                        if idx not in ref_ids:\n",
    "                            ref_ids.append(idx)#[\"int_prot_id\"])\n",
    "                            break\n",
    "    #matches is a tuple of strings used to filter reference sequences in pg_id_df; either \n",
    "    #one entry or gene_name and then the entry used by the GeneCards page \n",
    "    return ref_ids, matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fasta file reading functions: \n",
    "#filter_fasta_infile reads input files and outputs all records corresponding to filtered_ids to a new file\n",
    "#Remaining functions provide conversions between fasta files, pandas Series, and pandas dataframes \n",
    "#having alignment positions as columns \n",
    "\n",
    "def filter_fasta_infile(filtered_ids, infile_path, outfile_path=None,ordered=False):\n",
    "    #If outfile_path is provided, write filtered fasta to outfile_path\n",
    "    \"\"\"Generates new fasta file to outfile_path using the subset of sequences in infile_path\n",
    "    which have ids in filtered_ids\n",
    "    ordered: if true, sequences will be returned/ written in order of filtered_ids\n",
    "             if false, uses sequence order of sequences in infile_path\n",
    "    \"\"\"\n",
    "    def filtered_generator(filtered_ids, infile_path):\n",
    "        fasta_seqs = SeqIO.parse(open(infile_path),'fasta')\n",
    "        for fasta in fasta_seqs:\n",
    "            if fasta.id in filtered_ids:\n",
    "                yield fasta \n",
    "    def ordered_filtered_generator(filtered_ids, infile_path):\n",
    "        for id_ in filtered_ids:\n",
    "            fasta_seqs = SeqIO.parse(open(infile_path),'fasta')\n",
    "            for fasta in fasta_seqs:\n",
    "                if fasta.id == id_:\n",
    "                    yield fasta\n",
    "                    break\n",
    "    if outfile_path:\n",
    "        if ordered:\n",
    "            filtered = ordered_filtered_generator(filtered_ids, infile_path)\n",
    "        else:\n",
    "            filtered = filtered_generator(filtered_ids, infile_path)\n",
    "        SeqIO.write(filtered,outfile_path,\"fasta\")\n",
    "    if ordered:\n",
    "        filtered = ordered_filtered_generator(filtered_ids, infile_path)\n",
    "    else:\n",
    "        filtered = filtered_generator(filtered_ids, infile_path)\n",
    "    filtered_srs = pd.Series(index=filtered_ids)\n",
    "    for fasta in filtered:\n",
    "        filtered_srs[fasta.id] = str(fasta.seq)\n",
    "    return filtered_srs\n",
    "\n",
    "\n",
    "def srs_to_fasta(seq_srs, outfile_path):\n",
    "    #Write records in seq_srs to outfile_path in fasta format \n",
    "    def record_generator(seq_srs):\n",
    "        for idx, seq in seq_srs.iteritems():\n",
    "            record = SeqRecord(Seq(seq,IUPAC.protein),id=idx)\n",
    "            yield record\n",
    "    records = record_generator(seq_srs)\n",
    "    SeqIO.write(records,outfile_path,\"fasta\")\n",
    "\n",
    "def fasta_to_srs(fasta_path):\n",
    "    fasta_seqs = SeqIO.parse(open(fasta_path),'fasta')\n",
    "    id_seq_map = OrderedDict()\n",
    "    for fasta in fasta_seqs:\n",
    "        record_id = fasta.id\n",
    "        seq = str(fasta.seq)\n",
    "        id_seq_map[record_id] = seq\n",
    "    return pd.Series(name=\"seq\",data=id_seq_map)\n",
    "\n",
    "def align_srs_to_df(align_srs):\n",
    "    #Returns DataFrame object from series of aligned sequences; columns are 1-indexed positions\n",
    "    #Values are characters in alignment, index is ODB sequence IDs\n",
    "    n_seq = len(align_srs)\n",
    "#     display(align_srs)\n",
    "#     display(align_srs.iloc[0])\n",
    "    seq_len = len(align_srs.iloc[0])\n",
    "    align_df = pd.DataFrame(index=align_srs.index,columns=range(seq_len))\n",
    "    for idx, seq in align_srs.iteritems():\n",
    "        align_df.loc[idx,:] = list(seq)\n",
    "    align_df.columns += 1\n",
    "    return align_df\n",
    "\n",
    "def seq_srs_to_align_df(seq_srs,align_in_fpath,align_out_fpath):\n",
    "    \"\"\"Transform seq_srs (pandas Series containing sequence texts) to a DataFrame for which each column\n",
    "    is an alignment position and column. Writes input fasta and output fastas for alignment to align_in_fpath\n",
    "    and align_out_fpath respectively. Also returns average (non-diagonal) identity distances\"\"\"\n",
    "    srs_to_fasta(seq_srs,align_in_fpath)\n",
    "    n, ordered_ids, id_dm, align_srs = construct_id_dm(seq_srs,align_in_fpath,align_out_fpath)\n",
    "    align_df = align_srs_to_df(align_srs)\n",
    "    dist_srs = avg_dist_srs(align_srs.index,id_dm)\n",
    "    return align_df, dist_srs\n",
    "    \n",
    "def align_srs_to_seq_srs(align_srs,outfile_path=None):\n",
    "    #Return new Series (same index) of sequences with gap characters dropped\n",
    "    #If outfile_path is provided, write un-aligned record seqs to new fasta file\n",
    "    seq_srs = pd.Series(index=align_srs.index)\n",
    "    for idx, align_seq in align_srs.iteritems():\n",
    "        seq = align_seq.replace(\"-\",\"\")\n",
    "        seq_srs[idx] = seq\n",
    "    if outfile_path:\n",
    "        srs_to_fasta(seq_srs,outfile_path)\n",
    "    return seq\n",
    "\n",
    "def align_df_to_srs(align_df):\n",
    "    #Returns series of aligned sequences from array of aligned positions\n",
    "    align_srs = pd.Series(index=align_df.index)\n",
    "    for idx,record in align_df.iterrows():\n",
    "#       #seq is a string joining all characters with no delimiter (i.e. the original aligned sequence with gaps)\n",
    "        seq = ''.join(record.values)\n",
    "        align_srs[idx] = seq\n",
    "    return align_srs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ka_distmat(fasta_infile,align_outfile=\"tmp/ka_distmat_align.fasta\",distmat_file=\"tmp/ka_distmat.tsv\"):\n",
    "    \"\"\"REQUIRES: Modified KAlign source code to include distance matrix output, written to tmp/ka_distmat.tsv\n",
    "    Uses KAlign's modified distance metric (Wu-Manber, partial scoring for 3aa patterns with one error \n",
    "    tolerated), outputs to tsv and reads and stores as an ndarray (n x n) with n = number of sequences in fasta_infile\n",
    "    \n",
    "    Also return a list of the ODB ids of sequences (in order) corresponding to the distmat/ alignment order\n",
    "    \"\"\"\n",
    "    proc = subprocess.run(args=[\"kalign\",'-i',fasta_infile,\"-o\",align_outfile,\"-f\",\"fasta\"])\n",
    "    distmat_flines = open(distmat_file).readlines()\n",
    "    n = len(distmat_flines)\n",
    "    distmat = np.ndarray((n,n))\n",
    "    \n",
    "    for i,line in enumerate(distmat_flines):\n",
    "        as_list = line.split()\n",
    "        line_arr = np.array(as_list).astype(np.float)\n",
    "        distmat[i] = line_arr\n",
    "    ordered_ids = []\n",
    "    align_seqs = SeqIO.parse(open(align_outfile),'fasta')\n",
    "    for record in align_seqs:\n",
    "        ordered_ids.append(record.id)\n",
    "    return n, ordered_ids, distmat, align_outfile\n",
    "def construct_id_dm(seq_df,seq_fpath,align_outpath=\"tmp/iddm_align.fasta\",ordered=False):\n",
    "    \"\"\"\n",
    "    seq_df: DataFrame of OrthoDB/ NCBI sequence records; should only contain records for which identity \n",
    "    distance matrix will be computed \n",
    "    seq_fpath: Path of fasta file containing at least all of the records in seq_df. Can contain more records -\n",
    "    a temporary file containing only the records in seq_df.index will be generated \n",
    "    align_outpath: Optional filepath. If provided, the resulting alignment will be stored there. Otherwise,\n",
    "    written to a temporary file (tmp/iddm_align.fasta)\n",
    "    \n",
    "    Returns n (number of aligned sequences), ordered_ids (list of record ids in alignment), \n",
    "    id_dm (np.ndarray corresponding to the identity distance matrix computed by AlignIO), and align_srs\n",
    "    (pandas Series object containing aligned sequences)\n",
    "    \"\"\"\n",
    "    from Bio.Phylo.TreeConstruction import DistanceCalculator\n",
    "    from Bio import AlignIO\n",
    "    #Filter records in seq_fpath to new fasta only containing records in seq_df.index\n",
    "    filtered_outpath = \"tmp/iddm.fasta\"\n",
    "    filter_fasta_infile(seq_df.index,seq_fpath,outfile_path=filtered_outpath,ordered=ordered)\n",
    "    #KAlign sequences in filtered_outpath, write to align_outpath\n",
    "    n, ordered_ids, ka_dm, align_outfile = load_ka_distmat(filtered_outpath,align_outfile=align_outpath)\n",
    "    align_srs = fasta_to_srs(align_outpath)\n",
    "    aln = AlignIO.read(open(align_outpath), 'fasta')\n",
    "    calculator = DistanceCalculator('identity')\n",
    "    id_dm_obj = calculator.get_distance(aln)\n",
    "    #Convert AlignIO object to np.ndarray\n",
    "    for i,r in enumerate(id_dm_obj):\n",
    "        if i == 0:\n",
    "            id_dm = np.array(r)\n",
    "        else:\n",
    "            id_dm = np.vstack((id_dm,r))\n",
    "    return n, ordered_ids, id_dm, align_srs\n",
    "def avg_dist_srs(index,distmat):\n",
    "    #index is a pandas Index object with entries corresponding to the distmat (i.e. lengths should be equal)\n",
    "    #Calculate mean of non-self record distances (diagonal distances generally force-set to 0, so \n",
    "    #sum functions as intended)\n",
    "    n = len(distmat)\n",
    "    avg_dists = np.sum(distmat, axis=1)/(n-1)\n",
    "    dist_srs = pd.Series(data=avg_dists,index=index,name=\"dist\")\n",
    "    return dist_srs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ref_seqs4(gene_name,matches,ref_fasta_path,ref_df,seq_qc_fpath,known_spec_list=[\"10090_0\",\"9606_0\",\"43179_0\"]):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of records (at most one per species) corresponding to best internal matches for gene_name\n",
    "    \n",
    "    gene_name: gene symbol identifier \n",
    "    matches: set containing gene_name and GeneCards accepted alias for gene_name if different\n",
    "    ref_fasta_path: file path for fasta file containing all reference sequences \n",
    "    seq_qc_fpath: pseudo error log for records containing significant length differences or lacking supporting\n",
    "                  OrthoDB field information for matches with gene_name\n",
    "    known_spec_list: optional, contains tax_ids for species considered core to analysis \n",
    "    (generally species of interest and then well-annotated species i.e. human/mouse)\n",
    "    \n",
    "    Briefly: first records for human, mouse, and test species (13LGS) are filtered independently \n",
    "    with select_known_species_records, ultimately selecting a set of one record per species \n",
    "    that most consistently match the records (measured by percent identity in alignment) \n",
    "    present for the given gene symbol. \n",
    "    For other species, sequences with the highest identity to the established human/mouse/13LGS sequences\n",
    "    are selected but discarded if they do not meet the identity threshold (see select_species_records) or\n",
    "    if the length of the sequence is > 10% different from the median length. \n",
    "    \n",
    "    \"\"\"\n",
    "    ref_ids = list(ref_df.index)\n",
    "#     ksr_tsv_df = ref_tsv.loc[ref_tsv[\"organism_taxid\"].isin(known_spec_list)]\n",
    "    ksr_ref_df = ref_df.loc[ref_df[\"organism_taxid\"].isin(known_spec_list)]\n",
    "    #Create new tmp fasta file with known species annotated records only\n",
    "    ks_refseqs_fpath = \"tmp/ks_refseqs.fasta\"\n",
    "    known_spec_records = filter_fasta_infile(ksr_ref_df.index,ref_fasta_path,outfile_path=ks_refseqs_fpath)\n",
    "    \n",
    "    #Search ksr_full_df for pub_gene_id field matches to main aliases (matches)\n",
    "    upper_matches = [\"{0}$|{0}[;]\".format(match.upper()) for match in matches]\n",
    "    matches_pat = \"|\".join(upper_matches)\n",
    "    ksr_pgid_df = ksr_ref_df.loc[ksr_ref_df[\"pub_gene_id\"].str.upper().str.contains(matches_pat)]\n",
    "    final_ksr_df = select_known_species_records(ksr_pgid_df,ksr_ref_df,ks_refseqs_fpath)\n",
    "    #Quality checking of final_ksr_df:\n",
    "    final_ksr_df_QC(gene_name,matches,seq_qc_fpath,final_ksr_df,known_spec_list)\n",
    "    ref_pgid_df = ref_df.loc[ref_df[\"pub_gene_id\"].str.upper().str.contains(matches_pat)]\n",
    "    final_df = select_species_records(ref_pgid_df,ref_df,final_ksr_df,ref_fasta_path)\n",
    "    length_filter = True\n",
    "    if length_filter:\n",
    "        #Remove records whose length differs by more than 10% from the median (but keep representative seqs for \n",
    "        #human, mouse, GS)\n",
    "        med_len = final_df[\"length\"].median()\n",
    "        len_filtered = final_df.loc[(np.abs((final_df[\"length\"]-med_len)/med_len) < 0.1) | (final_df[\"organism_taxid\"].isin(known_spec_list)),:]\n",
    "        final_df = len_filtered\n",
    "#     final_align_df, dist_srs = seq_srs_to_align_df(final_df[\"seq\"],MSA_input_fasta,MSA_output_fasta)\n",
    "    return final_df\n",
    "\n",
    "def select_known_species_records(ksr_pgid_df,ksr_ref_df,ks_refseqs_fpath):\n",
    "    \"\"\"Returns a DataFrame of best records available for mouse, 13LGS, and human.\n",
    "    \n",
    "    ksr_pgid_df: DataFrame of known species (mouse, 13LGS, human) records for which pub_gene_id matched gene_name\n",
    "    ksr_ref_df: DataFrame of known species records meeting the match criteria established in find_ref_seqs\n",
    "    ks_refseqs_fpath: File path to a fasta containing the sequences corresponding to ksr_ref_df\n",
    "    \n",
    "    Return a dataframe of at most one record per species in KS_taxids, selecting first if there is only \n",
    "    one pubgene_id match record for that species and otherwise selecting the record with maximum identity\n",
    "    with other single pgid matched records, preferably from pgid_df but from ksr_ref_df if no pgid match exists\n",
    "    for that species.\n",
    "    If no species has only one record with a pubgene_id_match, takes manual user input to select a \n",
    "    representative record to base other record selection off of.\"\"\"\n",
    "    TEST_ID = \"43179_0\"\n",
    "    KS_TAXIDS = [\"10090_0\",\"43179_0\",\"9606_0\"]\n",
    "    ksr_taxid_uniques = ksr_ref_df[\"organism_taxid\"].unique()\n",
    "    pgid_taxid_uniques = ksr_pgid_df[\"organism_taxid\"].unique()\n",
    "    \n",
    "    #Case Handling: No human or mouse reference sequences for gene\n",
    "    if len(ksr_taxid_uniques) == 1 and TEST_ID in ksr_taxid_uniques:\n",
    "        raise SequenceDataError(3,\"No Human or Mouse reference sequences\")\n",
    "            \n",
    "    #Case Handling - if ksr_pgid_df is not composed of one sequence each for human, mouse, 13LGS\n",
    "    if len(ksr_pgid_df) > 3 or len(pgid_taxid_uniques) <3:\n",
    "        single_match_pgid_records = pd.DataFrame(columns=ksr_pgid_df.columns)\n",
    "        for ks_id in KS_TAXIDS:\n",
    "            ks_pgid_df = ksr_pgid_df.loc[ksr_pgid_df[\"organism_taxid\"]==ks_id,:]\n",
    "            if len(ks_pgid_df) == 1:\n",
    "                single_match_pgid_records = single_match_pgid_records.append(ks_pgid_df)\n",
    "                \n",
    "        if single_match_pgid_records.empty:\n",
    "            #Case handling if no single_match_records (ie CALM1): \n",
    "            #Allow manual input selection from either ksr_pgid_df or ksr_ref_df, raise SequenceDataError if both empty\n",
    "            if not ksr_pgid_df.empty: \n",
    "                print(\"pubgeneID matched records\")\n",
    "                display(ksr_pgid_df)\n",
    "                selection_df = ksr_pgid_df\n",
    "            elif not ksr_ref_df.empty:\n",
    "                print(\"GeneCards alias matched records\")\n",
    "                display(ksr_ref_df)\n",
    "                selection_df = ksr_ref_df\n",
    "            else:\n",
    "                raise SequenceDataError(2,\"No GeneCards alias matched sequence records for human/mouse/GS\")\n",
    "            try: \n",
    "                input_idx = input(\"Enter 0-indexed position of representative sequence for analysis\")\n",
    "                int_idx = int(input_idx)\n",
    "                selection_row = selection_df.iloc[int_idx,:]\n",
    "                \n",
    "            except (IndexError, ValueError) as e:\n",
    "                print(\"Bad Input\")\n",
    "                while (not re.search(\"^\\d+$\",input_idx)) or (int(input_idx)>=len(selection_df) or int(input_idx)<0):\n",
    "                    input_idx = input(\"Enter a number between 0 and {0}\".format(len(selection_df)-1))\n",
    "                int_idx = int(input_idx)\n",
    "                selection_row = selection_df.iloc[int_idx,:]\n",
    "            single_match_pgid_records = single_match_pgid_records.append(selection_row)\n",
    "\n",
    "        final_ksr_df = pd.DataFrame(columns=single_match_pgid_records.columns)\n",
    "        sm_record_ids = single_match_pgid_records.index                   \n",
    "            \n",
    "        for ks_id in KS_TAXIDS:\n",
    "            if ks_id not in single_match_pgid_records[\"organism_taxid\"].unique():\n",
    "            #For known_species taxids with 0 or 2+ pgid records: first check pgid matches (2+ pgid), then ref seqs (0 pgid)\n",
    "                if ks_id in pgid_taxid_uniques:\n",
    "                    #2+ pgid records: Construct id_dm of pgid matched records; select best ks_id \n",
    "                    #based on max identity with single_match_records\n",
    "                    n, ordered_ids, id_dm, align_srs = construct_id_dm(ksr_pgid_df,ks_refseqs_fpath)\n",
    "                elif ks_id in ks_id in ksr_taxid_uniques:\n",
    "                    #0 pgid records; construct id_dm from ksr_ref_df records, select max identity record\n",
    "                    n, ordered_ids, id_dm, align_srs = construct_id_dm(ksr_ref_df,ks_refseqs_fpath)\n",
    "                else:\n",
    "                    continue\n",
    "                #Maximum identity = minimum id_dm value based on AlignIO implementation\n",
    "                md_row, min_dist = min_dist_spec_record(ks_id,id_dm,ordered_ids,sm_record_ids,ksr_ref_df)\n",
    "                final_ksr_df = final_ksr_df.append(md_row)\n",
    "            else:\n",
    "                sm_row = single_match_pgid_records.loc[single_match_pgid_records[\"organism_taxid\"]==ks_id,:]\n",
    "                final_ksr_df = final_ksr_df.append(sm_row)\n",
    "        return final_ksr_df\n",
    "                \n",
    "    else:\n",
    "        #ksr_pgid_df has only one record each for mouse, 13LGS, human\n",
    "        #Sort ksr_pgid_df to order of taxids in KS_TAXIDS\n",
    "        final_ksr_df = pd.DataFrame(columns=ksr_pgid_df.columns)\n",
    "        for tax_id in KS_TAXIDS:\n",
    "            row = ksr_pgid_df.loc[ksr_pgid_df[\"organism_taxid\"]==tax_id,:]\n",
    "            final_ksr_df = final_ksr_df.append(row)\n",
    "        return final_ksr_df\n",
    "    \n",
    "def select_species_records(ref_pgid_df,ref_df,final_ksr_df,refseqs_fpath):\n",
    "    \"\"\"\n",
    "    Selects species records for secondary species (non human/ mouse/ 13LGS) from OrthoDB query input.\n",
    "    \n",
    "    ref_pgid_df: DataFrame of sequence records from OrthoDB for which pub_gene_id matched specified gene symbol\n",
    "    ref_df: DataFrame of sequence records from OrthoDB input which match specifications in find_ref_seqs\n",
    "    final_ksr_df: DataFrame of human/13LGS/mouse records selected by select_known_species_records\n",
    "    refseqs_fpath: Fasta file path for fasta containing all records in ref_df.\n",
    "    \n",
    "    First constructs alignment of final_ksr_df records, sets acceptable identity threshold to be the average of\n",
    "    non-diagonal entries in distance_matrix * 1.5. For each other species represented in ref_df, select minimum \n",
    "    distance record to HS/GS/MM sequences and add to final sequence set if it is under the identity threshold\n",
    "    \"\"\"\n",
    "    TEST_ID = \"43179_0\"\n",
    "    KS_TAXIDS = [\"10090_0\",\"43179_0\",\"9606_0\"]\n",
    "    pgid_taxids = [tax_id for tax_id in ref_pgid_df[\"organism_taxid\"].unique() if tax_id not in KS_TAXIDS]\n",
    "    ref_taxids = [tax_id for tax_id in ref_df[\"organism_taxid\"].unique() if tax_id not in KS_TAXIDS]\n",
    "    \n",
    "    #Distance calculations for final set of known species records - check internal identity values\n",
    "    #Set identity threshold - other species sequences above this value will not be included\n",
    "    ksr_dm_fpath = \"tmp/ksr_dm_ka.fasta\"\n",
    "    n, ksr_ordered_ids, ksr_id_dm, ksr_align_srs = construct_id_dm(final_ksr_df,refseqs_fpath,ksr_dm_fpath,ordered=True)\n",
    "    non_diagonal_avg = ksr_id_dm.sum(axis=0)/(n-1)\n",
    "    max_dist_idx = non_diagonal_avg.argmax()\n",
    "    max_dist_id = ksr_ordered_ids[max_dist_idx]\n",
    "    max_dist = non_diagonal_avg[max_dist_idx]\n",
    "    identity_threshold = np.mean(non_diagonal_avg)*1.5\n",
    "    \n",
    "    final_df = final_ksr_df.copy()\n",
    "    unfiltered_df = final_ksr_df.copy()\n",
    "    for tax_id in ref_taxids:\n",
    "        try:\n",
    "            tax_df = ref_df.loc[ref_df[\"organism_taxid\"]==tax_id]\n",
    "            tax_records = tax_df.index\n",
    "            #tax_dm_filtered_ids: list of record ids in final_ksr_df followed by all records corresponding to tax_id\n",
    "            tax_dm_filtered_ids = list(final_ksr_df.index)\n",
    "            tax_dm_filtered_ids.extend(tax_records) \n",
    "            tax_dm_df = ref_df.loc[tax_dm_filtered_ids,:]\n",
    "            \n",
    "            tax_dm_df = tax_dm_df.loc[~tax_dm_df.index.duplicated(keep='first')]\n",
    "            tax_dm_fpath = \"tmp/{0}_dm.fasta\".format(tax_id)\n",
    "            n, tax_ordered_ids, tax_id_dm, tax_align_srs = construct_id_dm(tax_dm_df,refseqs_fpath,tax_dm_fpath,ordered=True)\n",
    "            md_row, min_dist = min_dist_spec_record(tax_id,tax_id_dm,tax_ordered_ids,final_ksr_df.index,tax_dm_df)\n",
    "            if min_dist <= identity_threshold:\n",
    "                final_df = final_df.append(md_row)\n",
    "        except ValueError as e:\n",
    "            #Debugging edge case for OrthoDB data error with duplicate entries (Irf2bp2)\n",
    "            #Above code should handle duplicate entries sufficiently s.t. this block isn't run ever\n",
    "            #But who knows ‾\\_(ツ)_/‾\n",
    "            print(e)\n",
    "            display(tax_dm_df)\n",
    "            raise SequenceDataError(5,\"Duplicate Sequence Entry\")\n",
    "    return final_df    \n",
    "        \n",
    "def final_ksr_df_QC(gene_name,matches,seq_qc_fpath,final_ksr_df,ks_taxids):\n",
    "    \"\"\"Writes warnings about compiled final known species records to specified file path.\n",
    "    \n",
    "    Current behavior: writes to file if 1) any taxid from ks_taxids is not represented in final dataframe\n",
    "    2) if length of any individual record differs from the median length by >10% \n",
    "    3) if any record has a pub_gene_id that not represented by matches (ie LOC[XXXX...])\"\"\"\n",
    "    if len(final_ksr_df) < len(ks_taxids):\n",
    "        for tax_id in ks_taxids:\n",
    "            if tax_id not in final_ksr_df[\"organism_taxid\"].unique():\n",
    "                message_txt = \"No reference sequence for tax_id: {0}\".format(tax_id)\n",
    "                write_ref_seq_QC(seq_qc_fpath,gene_name,message_txt)\n",
    "    length_srs = final_ksr_df[\"length\"]\n",
    "    median_len = length_srs.median()\n",
    "    for record_id in final_ksr_df.index:\n",
    "        id_len = length_srs[record_id]\n",
    "        if (np.abs(id_len-median_len)/median_len) >= 0.1:\n",
    "            message_txt = \"Record_id {0} has length {1} which is greater than 10% different from the median ({2})\".format(record_id,id_len,median_len)\n",
    "            write_ref_seq_QC(seq_qc_fpath,gene_name,message_txt)\n",
    "    upper_matches = [match.upper() for match in matches]\n",
    "    upper_matches = [match+\"$|\"+match+\"[;]\" for match in upper_matches]\n",
    "    pat = \"|\".join(upper_matches)\n",
    "#     final_pgid_df = final_ksr_df.loc[final_ksr_df[\"pub_gene_id\"].str.upper().str.contains(pat)]\n",
    "    for record_id,pgid in final_ksr_df[\"pub_gene_id\"].iteritems():\n",
    "        if not re.search(pat,pgid.upper()):\n",
    "            message_txt = \"Record_id {0} has pub_gene_id {1} which doesn't match gene_name ({2})\".format(record_id,pgid,gene_name)\n",
    "            write_ref_seq_QC(seq_qc_fpath,gene_name,message_txt)\n",
    "def write_ref_seq_QC(seq_qc_fpath,gene_name,message):\n",
    "    \"\"\"Writes warning messages to seq_qc_fpath and prints (will not write duplicate entries)\"\"\"\n",
    "    if not os.path.exists(seq_qc_fpath):\n",
    "        seq_qc_f = open(seq_qc_fpath,'wt')\n",
    "        seq_qc_f.write(\"gene\\tmessage\\n\")\n",
    "    else:\n",
    "        seq_qc_f = open(seq_qc_fpath,'at')\n",
    "        seq_qc_df = pd.read_csv(seq_qc_fpath,delimiter='\\t')\n",
    "        if gene_name in seq_qc_df[\"gene\"].unique():\n",
    "            gene_df = seq_qc_df[seq_qc_df[\"gene\"]==gene_name]\n",
    "            if message not in gene_df[\"message\"].unique():\n",
    "                fline = \"{0}\\t{1}\\n\".format(gene_name,message)\n",
    "                seq_qc_f.write(fline)\n",
    "            else:\n",
    "                stored_message = seq_qc_df[seq_qc_df[\"gene\"]==gene_name][\"message\"].iloc[0]\n",
    "                print(\"Cached QC Warning:\")\n",
    "                message = stored_message\n",
    "        else:\n",
    "            fline = \"{0}\\t{1}\\n\".format(gene_name,message)\n",
    "            seq_qc_f.write(fline)\n",
    "    print(\"{0}: {1}\".format(gene_name,message))\n",
    "\n",
    "    \n",
    "def min_dist_spec_record(spec_taxid,distmat,dm_record_ids,accepted_record_ids, ref_df):\n",
    "    \"\"\"Returns row from ref_df which has minimum average distance against accepted records.\n",
    "    \n",
    "    spec_taxid: String identifier (ie prefix) that should be contained in all record ids specific to the\n",
    "    species of interest. Ex: 43179_, XP_ (if only one species has records from NCBI)\n",
    "    distmat: np.ndarray of pairwise distance values \n",
    "    dm_record_ids: ordered record ids corresponding to order of rows in distmat\n",
    "    accepted_record_ids: entries in dm_record_ids against which distance will be calculated in order to \n",
    "    select minimum distance entry.\n",
    "    ref_df: DataFrame containing record information. This function will return a row from this DataFrame\n",
    "    \n",
    "    Given a species taxid (spec_taxid), an np.ndarray distance matrix (distmat), a list of record_ids\n",
    "    corresponding to the rows in distmat (dm_record_ids),a list of accepted record ids (accepted_record_ids)\n",
    "    against which record distances will be averaged, and a DataFrame of sequences (ref_df):\n",
    "    Calculates the average distance of every record containing spec_taxid against accepted records, then\n",
    "    returns the row from ref_df corresponding to the record with lowest average distance\"\"\" \n",
    "    #TODO: Can update spec_records selection to function accepting list of species_record_ids rather than taxid\n",
    "    spec_records = [(i,id_) for i,id_ in enumerate(dm_record_ids) if re.search(spec_taxid,id_)]\n",
    "    spec_dm_idxs = [t[0] for t in spec_records] \n",
    "    accepted_records = [(i,id_) for i,id_ in enumerate(dm_record_ids) if id_ in accepted_record_ids]\n",
    "    accepted_dm_idxs = [t[0] for t in accepted_records]\n",
    "    #Select region of interest from DataFrame (ie columns corresponding to spec_records, rows \n",
    "    #corresponding to accepted_records)\n",
    "    spec_dm = distmat[:,spec_dm_idxs]\n",
    "    sub_dm = spec_dm[accepted_dm_idxs,:]\n",
    "    #Average into array of average distances for each record in spec_record\n",
    "    if len(sub_dm) > 1:\n",
    "        avg_dist = sub_dm.mean(axis=0)\n",
    "    else:\n",
    "        avg_dist = sub_dm[0]\n",
    "    min_idx = np.argmin(avg_dist)\n",
    "    min_dist_id = spec_records[min_idx][1]\n",
    "    min_dist = avg_dist[min_idx]\n",
    "    md_row = ref_df.loc[min_dist_id,:]\n",
    "    return md_row, min_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ODB_input(gene_name, run_name, test_species, errors_fpath, seq_qc_fpath,drop_spec_list=None):\n",
    "    #Generates final record and final alignment dataframes from raw input files. \n",
    "    #If errors arise (see write_errors), updates errors_fpath accordingly\n",
    "\n",
    "    input_fasta_fpath = \"{0}/input/{1}.fasta\".format(run_name,gene_name)\n",
    "    input_tsv_fpath = \"{0}/input/{1}.tsv\".format(run_name,gene_name)\n",
    "    gene_output_dir = \"{0}/output/{1}\".format(run_name,gene_name)\n",
    "    create_directory(gene_output_dir)\n",
    "    MSA_input_fasta = \"{0}/output/{1}/{1}.fasta\".format(run_name,gene_name)\n",
    "    MSA_output_fasta = \"{0}/output/{1}/{1}_MSA.fasta\".format(run_name,gene_name)\n",
    "    ref_fasta_path = \"tmp/ref_seqs.fasta\"\n",
    "    try: \n",
    "        tsv_df = pd.read_csv(input_tsv_fpath,delimiter='\\t')\n",
    "#         unfiltered_n = len(tsv_df[\"organism_taxid\"].unique())\n",
    "        tsv_df = tsv_df.set_index(keys=\"int_prot_id\",drop=True)#drop=False)\n",
    "    except (EmptyDataError, FileNotFoundError) as e:\n",
    "        raise OrthoDBQueryError(0,\"No OrthoDB results for query\")\n",
    "    if drop_spec_list:\n",
    "        tsv_df = tsv_df.loc[~tsv_df[\"organism_taxid\"].isin(drop_spec_list),:] \n",
    "    ref_ids, matches = find_ref_seqs(gene_name,tsv_df,errors_fpath)\n",
    "    if len(ref_ids) == 0:\n",
    "        raise SequenceDataError(0,\"No reference sequences could be found\")\n",
    "    elif test_species not in tsv_df[\"organism_name\"].values:\n",
    "        raise SequenceDataError(1,\"Test Species has no sequence in input\")\n",
    "    ref_tsv = tsv_df.loc[ref_ids,:]\n",
    "    \n",
    "    filtered_seq_srs = filter_fasta_infile(ref_ids,input_fasta_fpath,outfile_path=ref_fasta_path)\n",
    "    ref_seq_df = pd.DataFrame(filtered_seq_srs,columns=[\"seq\"])\n",
    "    ref_seq_df[\"length\"] = [len(seq) for seq in ref_seq_df[\"seq\"]]\n",
    "    ref_df = ref_tsv.join(ref_seq_df,how=\"inner\")\n",
    "    \n",
    "    if test_species not in ref_tsv[\"organism_name\"].values:\n",
    "        raise SequenceDataError(4,\"Test Species has no reference sequence in input (but a record is present in unfiltered ODB query)\")\n",
    "    final_records_df = filter_ref_seqs4(gene_name,matches,ref_fasta_path, ref_df,seq_qc_fpath)\n",
    "    final_align_df, dist_srs = seq_srs_to_align_df(final_records_df[\"seq\"],MSA_input_fasta,MSA_output_fasta)\n",
    "\n",
    "    return final_records_df, final_align_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse through AGS downloaded record files. Align to 13LGS, HS, MM sequences. If the record sharing\n",
    "#maximum identity to 13LGS and sharing max identity to HS/MM, display records and take user input\n",
    "#to resolve which to use. \n",
    "\n",
    "# ss_errors_fpath = \"{0}/summary/errors.tsv\".format(spec_subs_run_dir)\n",
    "# if os.path.exists(ss_errors_fpath):\n",
    "#     errors_df = pd.read_csv(ss_errors_fpath,sep='\\t',index_col='gene')\n",
    "#     data_errors = errors_df.loc[~(errors_df[\"error_type\"]==\"SequenceAnalysisError\"),:]\n",
    "#     data_errors_genes = data_errors.index\n",
    "def select_AGS_record(run_name,gene_name, final_ODB_records_df):\n",
    "    \"\"\n",
    "    KS_TAX_IDS = [\"10090_0\",\"43179_0\",\"9606_0\"]\n",
    "    TAX_IDS_13LGS = [\"43179_0\"]\n",
    "    TAX_IDS_HSMM = [\"10090_0\",\"9606_0\"]\n",
    "    TAX_ID_DICT = {\"Urocitellus parryii\":9999}\n",
    "    #Manual handling of inconsistent gene symbols between ODB and NCBI input files\n",
    "#     MISMATCHED_SYMBOLS = {\"TOIP1\":\"TOR1AIP1\",\"HNRH1\":\"HNRNPH1\",\"ATP5MC3\":\"ATP5G3\",\"ATP5G1 (ATP5MC1)\":\"ATP5G1\",\\\n",
    "#                          \"ATP5MC1\":\"ATP5G1\"}\n",
    "    REPEAT_MANUAL_SELECTION = False\n",
    "\n",
    "    #Generator function to yield SeqIO fasta objects from 1) ss_fasta_fpath corresponding to ksr_ids\n",
    "    #2) all AGS records in ags_fasta_fpath\n",
    "    def ksr_ags_generator(ksr_ids,ss_fasta_fpath,ags_fasta_fpath):\n",
    "        for id_ in ksr_ids:\n",
    "            ss_fastas = SeqIO.parse(open(ss_fasta_fpath),'fasta')\n",
    "            for fasta in ss_fastas:\n",
    "                if fasta.id == id_:\n",
    "                    yield fasta\n",
    "                    break\n",
    "        ags_fastas = SeqIO.parse(open(ags_fasta_fpath),'fasta')\n",
    "        for fasta in ags_fastas:\n",
    "            yield fasta\n",
    "\n",
    "    AGS_fasta_fpath = \"{0}/input/NCBI/{1}_AGS.fasta\"\n",
    "    ss_fasta_fpath = \"{0}/{1}/{1}.fasta\".format(output_dir,gene_name)\n",
    "    ss_records_fpath = \"{0}/{1}/{1}_records.csv\".format(output_dir,gene_name)\n",
    "    #Conditionally needed based on calling this function on gene set provided in AGS Gene ID file and not via SS\n",
    "#     if not os.path.exists(ss_fasta_fpath):\n",
    "#         if gene_name in data_errors.index:\n",
    "#             return\n",
    "#         elif gene_name in MISMATCHED_SYMBOLS:\n",
    "#             AGS_dir_gene_name = gene_name\n",
    "#             gene_name = MISMATCHED_SYMBOLS[gene_name]\n",
    "#             ss_fasta_fpath = \"{0}/{1}/{1}.fasta\".format(output_dir,gene_name)\n",
    "#             ss_records_fpath = \"{0}/{1}/{1}_records.csv\".format(output_dir,gene_name)\n",
    "#         else:\n",
    "#             dir_errors.append(gene_name)\n",
    "#             return\n",
    "#     if not os.path.exists(ss_fasta_fpath):\n",
    "#         \n",
    "        #cool\n",
    "        \n",
    "    ss_records_df = final_ODB_records_df.copy()\n",
    "    ks_records_df = ss_records_df.loc[ss_records_df[\"organism_taxid\"].isin(KS_TAX_IDS)]\n",
    "    \n",
    "    ks_record_ids = ks_records_df.index\n",
    "    ks_record_srs = filter_fasta_infile(ks_record_ids,ss_fasta_fpath,ordered=True)\n",
    "    ks_records_df[\"seq\"] = ks_record_srs\n",
    "    ags_fasta = SeqIO.parse(ags_fasta_fpath,'fasta')\n",
    "    #Process NCBI fasta file input into DataFrame with fields specified below \n",
    "    ags_fasta_df = pd.DataFrame(columns=[\"NCBI_id\",\"organism_taxid\",\"organism_name\",\"description\",\"length\",\"seq\"])\n",
    "    for f in ags_fasta:\n",
    "        organism_name = re.search(\"\\[(\\w+ \\w+)\\]\",f.description).groups()[0].strip()\n",
    "        organism_taxid = TAX_ID_DICT[organism_name]\n",
    "        trunc_desc = re.search(\"{0} (.*) \\[\".format(f.id), f.description).groups()[0].strip()\n",
    "        f_row = pd.Series({\"NCBI_id\":f.id,\"organism_taxid\":organism_taxid,\"organism_name\":organism_name,\\\n",
    "                           \"description\":trunc_desc,\"seq\":str(f.seq),\"length\":len(str(f.seq))},name=f.id)\n",
    "        ags_fasta_df = ags_fasta_df.append(f_row)\n",
    "    #Check for multiple AGS records - if more than one, identify AGS record with maximum identity to \n",
    "    #1) 13LGS and 2) homo sapiens/ mus musculus records\n",
    "    if len(ags_fasta_df) > 1:\n",
    "        ks_ags_idx = ks_records_df.index.copy().append(ags_fasta_df.index)\n",
    "        ks_ags_fasta_fpath = \"tmp/ks_ags_records.fasta\"\n",
    "        ks_ags_msa_fpath = \"tmp/ks_ags_records_MSA.fasta\"\n",
    "        fasta_generator = ksr_ags_generator(ks_record_ids,ss_fasta_fpath,ags_fasta_fpath)\n",
    "        SeqIO.write(fasta_generator,ks_ags_fasta_fpath,\"fasta\")\n",
    "        n,dm_record_ids,id_dm,align_srs = construct_id_dm(ks_ags_idx,ks_ags_fasta_fpath,align_outpath=ks_ags_msa_fpath)\n",
    "\n",
    "        spec_records = [(i,id_) for i,id_ in enumerate(dm_record_ids) if re.search(\"XP_\",id_)]\n",
    "        record_ids_13LGS, record_ids_hsmm = ks_records_df[ks_records_df[\"organism_taxid\"].isin(TAX_IDS_13LGS)].index, \\\n",
    "                                             ks_records_df[ks_records_df[\"organism_taxid\"].isin(TAX_IDS_HSMM)].index\n",
    "        md_row_13LGS, md_13LGS = min_dist_spec_record(\"XP_\",id_dm,dm_record_ids,record_ids_13LGS,ags_fasta_df)\n",
    "        md_row_hsmm, md_hsmm = min_dist_spec_record(\"XP_\",id_dm,dm_record_ids,record_ids_hsmm,ags_fasta_df)\n",
    "        \n",
    "        md13LGS_id = md_row_13LGS[\"NCBI_id\"]\n",
    "        mdhsmm_id = md_row_hsmm[\"NCBI_id\"]\n",
    "        if not md13LGS_id == mdhsmm_id and REPEAT_MANUAL_SELECTION:\n",
    "            with pd.option_context('display.max_colwidth', -1):\n",
    "                print(\"==={0}===\".format(gene_name))\n",
    "                print(\"Mouse (10090), 13LGS (43179), Human (9606) Records\")\n",
    "                display(ks_records_df)\n",
    "                print(\"AGS isoforms\")\n",
    "                display(ags_fasta_df)\n",
    "                print(\"Min Dist Row to 13LGS: \")\n",
    "                print(\"{0}\\t{1}\\t{2}\".format(md_row_13LGS[\"NCBI_id\"],md_row_13LGS[\"length\"],md_row_13LGS[\"seq\"]))\n",
    "                print(\"Identity to 13LGS: {0}\".format(md_13LGS))\n",
    "                print(\"Min Dist Row to HS, MM: \")\n",
    "                print(\"{0}\\t{1}\\t{2}\".format(md_row_hsmm[\"NCBI_id\"],md_row_hsmm[\"length\"],md_row_hsmm[\"seq\"]))\n",
    "                print(\"Identity to human, mouse: {0}\".format(md_hsmm))\n",
    "\n",
    "                row_idx = input(\"Enter 0-indexed position of record\")\n",
    "                while not re.search(\"^\\d+$\",row_idx) or (int(row_idx)<0 or int(row_idx)>=len(ags_fasta_df)):\n",
    "                    row_idx = input(\"Enter 0-indexed position of record\")\n",
    "                selection = ags_fasta_df.iloc[int(row_idx),:]\n",
    "                try: \n",
    "                    input_idx = input(\"Enter 0-indexed position of representative sequence for analysis\")\n",
    "                    int_idx = int(input_idx)\n",
    "                    selection_row = ags_fasta_df.iloc[int_idx,:]\n",
    "                except (IndexError, ValueError) as e:\n",
    "                    print(\"Bad Input\")\n",
    "                    while (not re.search(\"^\\d+$\",input_idx)) or (int(input_idx)>=len(selection_df) or int(input_idx)<0):\n",
    "                        input_idx = input(\"Enter a number between 0 and {0}\".format(len(selection_df)-1))\n",
    "                    int_idx = int(input_idx)\n",
    "                    selection_row = ags_fasta_df.iloc[int_idx,:]\n",
    "\n",
    "                manual_selections[gene_name] = selection_row\n",
    "#         elif gene_name in DISPLAY_RECORDS or gene_name in OVERWRITE_FASTAS:\n",
    "        elif gene_name in OVERWRITE_FASTAS:\n",
    "            print(\"==={0}===\".format(gene_name))\n",
    "            print(\"Mouse (10090), 13LGS (43179), Human (9606) Records\")\n",
    "            display(ks_records_df)\n",
    "            print(\"AGS isoforms\")\n",
    "            display(ags_fasta_df)\n",
    "\n",
    "            print(\"Min Dist Row to 13LGS: \")\n",
    "            print(\"{0}\\t{1}\\t{2}\".format(md_row_13LGS[\"NCBI_id\"],md_row_13LGS[\"length\"],md_row_13LGS[\"seq\"]))\n",
    "            print(\"Identity to 13LGS: {0}\".format(md_13LGS))\n",
    "            print(\"Min Dist Row to HS, MM: \")\n",
    "            print(\"{0}\\t{1}\\t{2}\".format(md_row_hsmm[\"NCBI_id\"],md_row_hsmm[\"length\"],md_row_hsmm[\"seq\"]))\n",
    "            print(\"Identity to human, mouse: {0}\".format(md_hsmm))\n",
    "            \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
